{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24716739-3470-40c9-b83b-710c29ed5bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df['lemmatized_text'] ======  0       [new, upcoming, hero, reveal, wrong, dragon, omg]\n",
      "1         [lunchtime, flinthook, game, much, personality]\n",
      "2                                     [hype, real, happy]\n",
      "3       [knight, day, jewellery, end, sundaywhy, not, ...\n",
      "4                             [thats, law, nature, happy]\n",
      "                              ...                        \n",
      "2640                                       [whaddup, cry]\n",
      "2641    [want, give, everything, make, happy, see, smi...\n",
      "2642    [dont, want, tell, fellow, comm, student, dont...\n",
      "2643                                [year, haha, unhappy]\n",
      "2644    [happy, birthday, katrina, miss, though, unhap...\n",
      "Name: lemmatized_text, Length: 2645, dtype: object\n",
      "Top Positive Words: [('happy', 573), ('thanks', 95), ('want', 65), ('get', 57), ('great', 56), ('follow', 55), ('much', 54), ('smile', 54), ('love', 52), ('good', 46)]\n",
      "Top Negative Words: [('unhappy', 688), ('get', 71), ('sad', 69), ('miss', 66), ('not', 63), ('want', 60), ('im', 55), ('dont', 53), ('like', 53), ('go', 53)]\n",
      "Top Neutral Words: [('also', 78), ('epaper', 75), ('court', 65), ('say', 64), ('india', 53), ('govt', 50), ('minister', 45), ('not', 40), ('supreme', 39), ('take', 37)]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93       180\n",
      "           2       0.83      0.99      0.90       188\n",
      "           4       0.93      0.80      0.86       161\n",
      "\n",
      "    accuracy                           0.90       529\n",
      "   macro avg       0.91      0.90      0.90       529\n",
      "weighted avg       0.91      0.90      0.90       529\n",
      "\n",
      "Confusion Matrix:\n",
      "[[161  11   8]\n",
      " [  0 187   1]\n",
      " [  4  28 129]]\n",
      "Accuracy: 90.17013232514176 %\n",
      "Testing: tokenization + binary\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 215\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Run the Test\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_combinations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Display Results\u001b[39;00m\n\u001b[1;32m    218\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing Technique\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Preparation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[12], line 205\u001b[0m, in \u001b[0;36mtest_combinations\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m methods:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtechnique\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m + \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtechnique\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     X \u001b[38;5;241m=\u001b[39m prepare_features(processed_text, method)\n\u001b[1;32m    207\u001b[0m     y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(sentiment_to_numbers)\n",
      "Cell \u001b[0;32mIn[12], line 160\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text, technique)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text, technique):\n\u001b[0;32m--> 160\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize_repeated_letters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mapply(word_tokenize)\n\u001b[1;32m    162\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mapply(remove_stop_words)\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m, in \u001b[0;36mnormalize_repeated_letters\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_repeated_letters\u001b[39m(text):\n\u001b[0;32m---> 48\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(.)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m2,}\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Spell(text)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/__init__.py:208\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags, *args)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is passed as positional argument\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    206\u001b[0m     )\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install and Import Libraries\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from collections import Counter\n",
    "from autocorrect import Speller\n",
    "import re\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Cell 2: Download NLTK Datasets (Only if Not Already Downloaded)\n",
    "import nltk\n",
    "\n",
    "# Check if NLTK datasets are already downloaded\n",
    "nltk_resources = [\"stopwords\", \"punkt\", \"wordnet\", \"averaged_perceptron_tagger\"]\n",
    "\n",
    "for resource in nltk_resources:\n",
    "    try:\n",
    "        # Check if the resource is already downloaded\n",
    "        nltk.data.find(f\"corpora/{resource}\")\n",
    "    except LookupError:\n",
    "        # If not, download it quietly\n",
    "        nltk.download(resource, quiet=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "Spell = Speller()\n",
    "stemmer = PorterStemmer()\n",
    "folder_path = \"/Users/reroreo1/Desktop/p00_tweets/\"\n",
    "files = ['processedPositive.csv', 'processedNeutral.csv', 'processedNegative.csv']\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Cell 3: Define Preprocessing Functions\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\d+\", \"\", text.lower()).replace(\"'\", \"\").replace('\"', \"\")\n",
    "    text = re.sub(r\",(?=\\S)\", \"\\n\", text)\n",
    "    return [line.strip() for line in text.split(\"\\n\") if len(line.split()) >= 2]\n",
    "\n",
    "def normalize_repeated_letters(text):\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return Spell(text)\n",
    "\n",
    "negations = {\"not\", \"no\", \"nor\", \"never\"}\n",
    "\n",
    "def get_most_frequent_words(df, sentiment, n=10):\n",
    "    \"\"\"\n",
    "    Get the most frequent words in tweets of a specific sentiment.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing 'text' and 'sentiment'.\n",
    "        sentiment (str): The sentiment category (e.g., 'positive').\n",
    "        n (int): The number of top words to return.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: Top `n` words and their frequencies.\n",
    "    \"\"\"\n",
    "    word_counter = Counter()\n",
    "    # Filter tweets by sentiment and update word frequencies\n",
    "    df[df['sentiment'] == sentiment]['lem_word_count'].apply(lambda x: word_counter.update(x))\n",
    "    return word_counter.most_common(n)\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    return [word for word in words if word not in stop_words or word in negations]\n",
    "\n",
    "def stemmatization(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def sentiment_to_numbers(text):\n",
    "    if text == \"positive\":\n",
    "        return 4\n",
    "    elif text == \"negative\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_pos(words):\n",
    "    tags = nltk.pos_tag(words)\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tags]\n",
    "\n",
    "def lemmatize_pos_for_word_count(words):\n",
    "    tags = nltk.pos_tag(words)\n",
    "    lem_list = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tags]\n",
    "    return lem_list\n",
    "\n",
    "# Cell 4: Process All Files\n",
    "tweets = {}\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    text = clean_text(raw_text)\n",
    "    processed_text = list(set(text))\n",
    "    key = file.replace(\"processed\", \"\").replace(\".csv\", \"\").lower()\n",
    "    tweets[key] = processed_text\n",
    "\n",
    "# Cell 5: Prepare DataFrame\n",
    "data = []\n",
    "for sentiment, tweet_list in tweets.items():\n",
    "    for tweet in tweet_list:\n",
    "        data.append((tweet, sentiment))\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"sentiment\"])\n",
    "df[\"text\"] = df[\"text\"].apply(normalize_repeated_letters)\n",
    "df[\"text\"] = df[\"text\"].apply(word_tokenize)\n",
    "df[\"text\"] = df[\"text\"].apply(remove_stop_words)\n",
    "df['lemmatized_text'] = df[\"text\"].apply(lemmatize_pos)\n",
    "print(\"df['lemmatized_text'] ====== \" , df['lemmatized_text'])\n",
    "\n",
    "# Cell 6: Analyze Most Frequent Words\n",
    "df['lem_word_count'] = df[\"text\"].apply(lemmatize_pos_for_word_count)\n",
    "\n",
    "positive_words = get_most_frequent_words(df, 'positive', 10)\n",
    "negative_words = get_most_frequent_words(df, 'negative', 10)\n",
    "neutral_words = get_most_frequent_words(df, 'neutral', 10)\n",
    "\n",
    "print(\"Top Positive Words:\", positive_words)\n",
    "print(\"Top Negative Words:\", negative_words)\n",
    "print(\"Top Neutral Words:\", neutral_words)\n",
    "\n",
    "# Cell 7: Train and Evaluate Logistic Regression Model\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "df['lem_text_str'] = df['lemmatized_text'].apply(lambda x: ' '.join(x))\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['lem_text_str'])\n",
    "y = df['sentiment'].apply(sentiment_to_numbers)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, y, test_size=0.2, random_state=42)\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100} %\")\n",
    "\n",
    "# Cell 8: Test All Preprocessing and Feature Preparation Combinations\n",
    "def preprocess_text(text, technique):\n",
    "    text = text.apply(normalize_repeated_letters)\n",
    "    text = text.apply(word_tokenize)\n",
    "    text = text.apply(remove_stop_words)\n",
    "    \n",
    "    if technique == \"tokenization\":\n",
    "        return text.apply(lambda x: ' '.join(x))\n",
    "    elif technique == \"stemming\":\n",
    "        return text.apply(lambda x: ' '.join(stemmatization(x)))\n",
    "    elif technique == \"lemmatization\":\n",
    "        return text.apply(lambda x: ' '.join(lemmatize_pos(x)))\n",
    "    elif technique == \"stemming+lemmatization\":\n",
    "        stemmed = text.apply(lambda x: stemmatization(x))\n",
    "        return stemmed.apply(lambda x: ' '.join(lemmatize_pos(x)))\n",
    "    elif technique == \"misspellings\":\n",
    "        return text.apply(lambda x: ' '.join([Spell(word) for word in x]))\n",
    "    elif technique == \"lemmatization+misspellings\":\n",
    "        lemmatized = text.apply(lambda x: lemmatize_pos(x))\n",
    "        return lemmatized.apply(lambda x: ' '.join([Spell(word) for word in x]))\n",
    "    else:\n",
    "        return text.apply(lambda x: ' '.join(x))\n",
    "\n",
    "def prepare_features(text, method):\n",
    "    if method == \"binary\":\n",
    "        vectorizer = CountVectorizer(binary=True, max_features=5000)\n",
    "    elif method == \"word_counts\":\n",
    "        vectorizer = CountVectorizer(max_features=5000)\n",
    "    elif method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    return vectorizer.fit_transform(text)\n",
    "\n",
    "def evaluate_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "def test_combinations(df):\n",
    "    results = []\n",
    "    techniques = [\"tokenization\", \"stemming\", \"lemmatization\", \"stemming+lemmatization\", \"misspellings\", \"lemmatization+misspellings\"]\n",
    "    methods = [\"binary\", \"word_counts\", \"tfidf\"]\n",
    "    \n",
    "    for technique in techniques:\n",
    "        for method in methods:\n",
    "            print(f\"Testing: {technique} + {method}\")\n",
    "            processed_text = preprocess_text(df[\"text\"], technique)\n",
    "            X = prepare_features(processed_text, method)\n",
    "            y = df['sentiment'].apply(sentiment_to_numbers)\n",
    "            accuracy = evaluate_model(X, y)\n",
    "            results.append((technique, method, accuracy))\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the Test\n",
    "results = test_combinations(df)\n",
    "\n",
    "# Display Results\n",
    "results_df = pd.DataFrame(results, columns=[\"Preprocessing Technique\", \"Feature Preparation\", \"Accuracy\"])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddc5b6-187c-4cd9-a616-8aff806e30dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sentiment_env)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
