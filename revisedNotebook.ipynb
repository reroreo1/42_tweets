{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22d92903-2d3e-4cc7-afb2-75b577deca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reroreo1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/reroreo1/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/reroreo1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/reroreo1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from autocorrect import Speller\n",
    "import re\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "pd\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "Spell = Speller()\n",
    "stemmer = PorterStemmer()\n",
    "folder_path = \"./p00_tweets/\"\n",
    "files = ['processedPositive.csv', 'processedNeutral.csv', 'processedNegative.csv']\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91e0af4e-3aeb-4729-8269-eebdde824421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase, remove numbers, quotes, and strip leading/trailing spaces\n",
    "    text = re.sub(r\"\\d+\", \"\", text.lower()).replace(\"'\", \"\").replace('\"', \"\")\n",
    "    # Add newlines where a comma is not followed by a space\n",
    "    text = re.sub(r\",(?=\\S)\", \"\\n\", text)\n",
    "    # Split by lines and filter out short lines (less than 2 words)\n",
    "    return [line.strip() for line in text.split(\"\\n\") if len(line.split()) >= 2]\n",
    "    \n",
    "def normalize_repeated_letters_and_remove_non_alpha_char(text):\n",
    "    # Replace 3+ repeated letters with a single occurrence\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)  # e.g., \"happyyyy\" -> \"happy\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)   # Remove non-alphabetic characters\n",
    "    return text\n",
    "    \n",
    "negations = {\"not\", \"no\", \"nor\", \"never\"}\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    cleaned_list = [word for word in words if word not in stop_words or word in negations]\n",
    "    return cleaned_list\n",
    "    \n",
    "def count_words(tweet):\n",
    "    return Counter(tweet)\n",
    "\n",
    "def stemmatization(words):\n",
    "    stem_list = [stemmer.stem(word) for word in words]\n",
    "    return stem_list\n",
    "\n",
    "def sentiment_to_numbers(text):\n",
    "    if text == \"positive\":\n",
    "        return 4\n",
    "    elif text == \"negative\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):  # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):  # Verb\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):  # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):  # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to count words for each sentiment category\n",
    "# def get_most_frequent_words(df, sentiment, n=10):\n",
    "#     \"\"\"\n",
    "#     Get the most frequent words in tweets of a specific sentiment.\n",
    "\n",
    "#     Args:\n",
    "#         df (DataFrame): The DataFrame containing 'text' and 'sentiment'.\n",
    "#         sentiment (str): The sentiment category (e.g., 'positive').\n",
    "#         n (int): The number of top words to return.\n",
    "\n",
    "#     Returns:\n",
    "#         List of tuples: Top `n` words and their frequencies.\n",
    "#     \"\"\"\n",
    "#     word_counter = Counter()\n",
    "#     # Filter tweets by sentiment and update word frequencies\n",
    "#     df[df['sentiment'] == sentiment]['lem_word_count'].apply(lambda x: word_counter.update(x))\n",
    "#     return word_counter.most_common(n)\n",
    "\n",
    "# Find the top 10 most frequent words for each sentiment\n",
    "\n",
    "def lemmatize_pos(words):\n",
    "    # words = words.apply(word_tokenize)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    lem_list = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tags]\n",
    "    return lem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46743a1d-2b1e-41f8-9b36-62e9399912b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = {}\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    # Clean the text and remove duplicates\n",
    "    text = clean_text(raw_text)\n",
    "    processed_text = list(set(text))\n",
    "    # Save processed tweets in the dictionary\n",
    "    key = file.replace(\"processed\", \"\").replace(\".csv\", \"\").lower()\n",
    "    tweets[key] = processed_text\n",
    "\n",
    "    \n",
    "\n",
    "# Output: A dictionary with cleaned and filtered tweets\n",
    "data = []\n",
    "for sentiment, tweet_list in tweets.items():\n",
    "    for tweet in tweet_list:\n",
    "        data.append((tweet, sentiment))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"sentiment\"])\n",
    "\n",
    "# df['lemmatized_text'] = df[\"text\"].apply(lemmatize_pos)\n",
    "\n",
    "# df['lem_word_count'] = df[\"text\"].apply(lemmatize_pos_for_word_count)\n",
    "\n",
    "# positive_words = get_most_frequent_words(df, 'positive', 10)\n",
    "# negative_words = get_most_frequent_words(df, 'negative', 10)\n",
    "# neutral_words = get_most_frequent_words(df, 'neutral', 10)\n",
    "\n",
    "# # Display the results\n",
    "# print(\"Top Positive Words:\", positive_words)\n",
    "# print(\"Top Negative Words:\", negative_words)\n",
    "# print(\"Top Neutral Words:\", neutral_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1215af97-1654-4ebe-9c7e-84f1e8bd142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, technique):\n",
    "    if \"custom preprocessing\" in technique:\n",
    "        text = text.apply(normalize_repeated_letters_and_remove_non_alpha_char)\n",
    "    text = text.apply(word_tokenize)\n",
    "    if \"custom preprocessing\" in technique:\n",
    "        text = text.apply(remove_stop_words)\n",
    "    # print(f\"preprocessText: {text}\")\n",
    "    if technique == \"tokenization\":\n",
    "        return text.apply(lambda x: ' '.join(x))\n",
    "    elif technique == \"stemming\":\n",
    "        return text.apply(lambda x: ' '.join(stemmatization(x)))\n",
    "    elif technique == \"lemmatization\":\n",
    "        return text.apply(lambda x: ' '.join(lemmatize_pos(x)))\n",
    "    elif technique == \"stemming + misspellings\" or technique == \"stemming + misspellings + custom preprocessing\":\n",
    "        stemmed = text.apply(lambda x: stemmatization(x))\n",
    "        return text.apply(lambda x: ' '.join([Spell(word) for word in x]))\n",
    "    elif technique == \"lemmatization + misspellings\" or technique == \"lemmatization + misspellings + custom preprocessing\":\n",
    "        lemmatized = text.apply(lambda x: lemmatize_pos(x))\n",
    "        return lemmatized.apply(lambda x: ' '.join([Spell(word) for word in x]))\n",
    "    else:\n",
    "        return text.apply(lambda x: ' '.join(x))\n",
    "\n",
    "def prepare_features(text, method):\n",
    "    if method == \"binary\":\n",
    "        vectorizer = CountVectorizer(binary=True, max_features=5000)\n",
    "    elif method == \"word_counts\":\n",
    "        vectorizer = CountVectorizer(max_features=5000)\n",
    "    elif method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    return vectorizer.fit_transform(text)\n",
    "\n",
    "   \n",
    "def evaluate_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression(max_iter=5000,random_state=42)  # saga is better for sparse data\n",
    "    param_grid = [{'C': 10**np.linspace(-3, 3, 20), 'solver': ['lbfgs', 'saga', 'liblinear']}]\n",
    "    logreg_cv=GridSearchCV(logreg,param_grid, cv=10, scoring='accuracy', \n",
    "                             refit=True)\n",
    "    logreg_cv.fit(X_train,y_train)\n",
    "    print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "    y_pred = logreg_cv.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"accuracy_score function - Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "    \n",
    "def test_combinations(df):\n",
    "    results = []\n",
    "    techniques = [\"tokenization\", \"stemming\", \"lemmatization\", \"stemming + misspellings\", \"lemmatization + misspellings\",\"lemmatization + misspellings + custom preprocessing\",\"stemming + misspellings + custom preprocessing\"]\n",
    "                  \n",
    "    methods = [\"binary\", \"word_counts\", \"tfidf\"]\n",
    "    \n",
    "    for technique in techniques:\n",
    "        for method in methods:\n",
    "            print(f\"Testing: {technique} + {method}\")\n",
    "            processed_text = preprocess_text(df[\"text\"], technique)\n",
    "            X = prepare_features(processed_text, method)\n",
    "            y = df['sentiment'].apply(sentiment_to_numbers)\n",
    "            accuracy = evaluate_model(X, y)\n",
    "            results.append((technique, method, accuracy))\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94031188-8461-4d8b-94c3-9ccb5dfd2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: tokenization + binary\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663), 'solver': 'saga'}\n",
      "accuracy_score function - Accuracy: 0.9187145557655955\n",
      "Accuracy: 0.9187\n",
      "Testing: tokenization + word_counts\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663), 'solver': 'lbfgs'}\n",
      "accuracy_score function - Accuracy: 0.9243856332703214\n",
      "Accuracy: 0.9244\n",
      "Testing: tokenization + tfidf\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(6.158482110660261), 'solver': 'lbfgs'}\n",
      "accuracy_score function - Accuracy: 0.9168241965973535\n",
      "Accuracy: 0.9168\n",
      "Testing: stemming + binary\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(0.6951927961775606), 'solver': 'lbfgs'}\n",
      "accuracy_score function - Accuracy: 0.9262759924385633\n",
      "Accuracy: 0.9263\n",
      "Testing: stemming + word_counts\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663), 'solver': 'lbfgs'}\n",
      "accuracy_score function - Accuracy: 0.9187145557655955\n",
      "Accuracy: 0.9187\n",
      "Testing: stemming + tfidf\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(26.366508987303554), 'solver': 'liblinear'}\n",
      "accuracy_score function - Accuracy: 0.9149338374291115\n",
      "Accuracy: 0.9149\n",
      "Testing: lemmatization + binary\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663), 'solver': 'saga'}\n",
      "accuracy_score function - Accuracy: 0.9187145557655955\n",
      "Accuracy: 0.9187\n",
      "Testing: lemmatization + word_counts\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(112.88378916846884), 'solver': 'liblinear'}\n",
      "accuracy_score function - Accuracy: 0.9111531190926276\n",
      "Accuracy: 0.9112\n",
      "Testing: lemmatization + tfidf\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(233.57214690901213), 'solver': 'lbfgs'}\n",
      "accuracy_score function - Accuracy: 0.9206049149338374\n",
      "Accuracy: 0.9206\n",
      "Testing: stemming + misspellings + binary\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663), 'solver': 'saga'}\n",
      "accuracy_score function - Accuracy: 0.9224952741020794\n",
      "Accuracy: 0.9225\n",
      "Testing: stemming + misspellings + word_counts\n",
      "tuned hpyerparameters :(best parameters)  {'C': np.float64(6.158482110660261), 'solver': 'liblinear'}\n",
      "accuracy_score function - Accuracy: 0.9262759924385633\n",
      "Accuracy: 0.9263\n",
      "Testing: stemming + misspellings + tfidf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the Test\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_combinations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Display Results\u001b[39;00m\n\u001b[1;32m      6\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing Technique\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Preparation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[46], line 57\u001b[0m, in \u001b[0;36mtest_combinations\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m methods:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtechnique\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m + \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtechnique\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     X \u001b[38;5;241m=\u001b[39m prepare_features(processed_text, method)\n\u001b[1;32m     59\u001b[0m     y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(sentiment_to_numbers)\n",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text, technique)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstemming + misspellings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstemming + misspellings + custom preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     15\u001b[0m     stemmed \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: stemmatization(x))\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSpell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatization + misspellings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatization + misspellings + custom preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     18\u001b[0m     lemmatized \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: lemmatize_pos(x))\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m, in \u001b[0;36mpreprocess_text.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstemming + misspellings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstemming + misspellings + custom preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     15\u001b[0m     stemmed \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: stemmatization(x))\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43mSpell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x]))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatization + misspellings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatization + misspellings + custom preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     18\u001b[0m     lemmatized \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: lemmatize_pos(x))\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autocorrect/__init__.py:128\u001b[0m, in \u001b[0;36mSpeller.autocorrect_sentence\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautocorrect_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mword_regexes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocorrect_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/re/__init__.py:208\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags, *args)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is passed as positional argument\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    206\u001b[0m     )\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autocorrect/__init__.py:130\u001b[0m, in \u001b[0;36mSpeller.autocorrect_sentence.<locals>.<lambda>\u001b[0;34m(match)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautocorrect_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    129\u001b[0m         word_regexes[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang],\n\u001b[0;32m--> 130\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m match: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocorrect_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    131\u001b[0m         sentence,\n\u001b[1;32m    132\u001b[0m     )\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autocorrect/__init__.py:114\u001b[0m, in \u001b[0;36mSpeller.autocorrect_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# in case the word is capitalized\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39misupper():\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autocorrect/__init__.py:104\u001b[0m, in \u001b[0;36mSpeller.get_candidates\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     99\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting([word]) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting(w\u001b[38;5;241m.\u001b[39mtypos()) \u001b[38;5;129;01mor\u001b[39;00m [word]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting([word])\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting(w\u001b[38;5;241m.\u001b[39mtypos())\n\u001b[0;32m--> 104\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexisting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_typos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m [word]\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_data\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0\u001b[39m), c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autocorrect/__init__.py:94\u001b[0m, in \u001b[0;36mSpeller.existing\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexisting\u001b[39m(\u001b[38;5;28mself\u001b[39m, words):\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"{'the', 'teh'} => {'the'}\"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_data}\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autocorrect/typos.py:56\u001b[0m, in \u001b[0;36mWord._replaces\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet:\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the Test\n",
    "\n",
    "results = test_combinations(df)\n",
    "\n",
    "# Display Results\n",
    "results_df = pd.DataFrame(results, columns=[\"Preprocessing Technique\", \"Feature Preparation\", \"Accuracy\"])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f036c2-6f69-4c93-8c84-dfb5e0175a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: tokenization + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(0.6951927961775606)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: tokenization + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: tokenization + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(6.158482110660261)}\n",
    "# accuracy_score function - Accuracy: 0.9168241965973535\n",
    "# Accuracy: 0.9168\n",
    "# Testing: stemming + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(0.6951927961775606)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: stemming + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9187145557655955\n",
    "# Accuracy: 0.9187\n",
    "# Testing: stemming + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(12.742749857031322)}\n",
    "# accuracy_score function - Accuracy: 0.9187145557655955\n",
    "# Accuracy: 0.9187\n",
    "# Testing: lemmatization + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: lemmatization + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(0.6951927961775606)}\n",
    "# accuracy_score function - Accuracy: 0.9319470699432892\n",
    "# Accuracy: 0.9319\n",
    "# Testing: lemmatization + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(233.57214690901213)}\n",
    "# accuracy_score function - Accuracy: 0.9206049149338374\n",
    "# Accuracy: 0.9206\n",
    "# Testing: stemming + misspellings + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: stemming + misspellings + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: stemming + misspellings + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9187145557655955\n",
    "# Accuracy: 0.9187\n",
    "# Testing: lemmatization + misspellings + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: lemmatization + misspellings + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(6.158482110660261)}\n",
    "# accuracy_score function - Accuracy: 0.9206049149338374\n",
    "# Accuracy: 0.9206\n",
    "# Testing: lemmatization + misspellings + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1000.0)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: lemmatization + misspellings + custom preprocessing + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: lemmatization + misspellings + custom preprocessing + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(6.158482110660261)}\n",
    "# accuracy_score function - Accuracy: 0.9206049149338374\n",
    "# Accuracy: 0.9206\n",
    "# Testing: lemmatization + misspellings + custom preprocessing + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1000.0)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: stemming + misspellings + custom preprocessing + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: stemming + misspellings + custom preprocessing + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: stemming + misspellings + custom preprocessing + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9187145557655955\n",
    "# Accuracy: 0.9187\n",
    "#                               Preprocessing Technique Feature Preparation  \\\n",
    "# 0                                        tokenization              binary   \n",
    "# 1                                        tokenization         word_counts   \n",
    "# 2                                        tokenization               tfidf   \n",
    "# 3                                            stemming              binary   \n",
    "# 4                                            stemming         word_counts   \n",
    "# 5                                            stemming               tfidf   \n",
    "# 6                                       lemmatization              binary   \n",
    "# 7                                       lemmatization         word_counts   \n",
    "# 8                                       lemmatization               tfidf   \n",
    "# 9                             stemming + misspellings              binary   \n",
    "# 10                            stemming + misspellings         word_counts   \n",
    "# 11                            stemming + misspellings               tfidf   \n",
    "# 12                       lemmatization + misspellings              binary   \n",
    "# 13                       lemmatization + misspellings         word_counts   \n",
    "# 14                       lemmatization + misspellings               tfidf   \n",
    "# 15  lemmatization + misspellings + custom preproce...              binary   \n",
    "# 16  lemmatization + misspellings + custom preproce...         word_counts   \n",
    "# 17  lemmatization + misspellings + custom preproce...               tfidf   \n",
    "# 18     stemming + misspellings + custom preprocessing              binary   \n",
    "# 19     stemming + misspellings + custom preprocessing         word_counts   \n",
    "# 20     stemming + misspellings + custom preprocessing               tfidf   \n",
    "\n",
    "#     Accuracy  \n",
    "# 0   0.926276  \n",
    "# 1   0.924386  \n",
    "# 2   0.916824  \n",
    "# 3   0.926276  \n",
    "# 4   0.918715  \n",
    "# 5   0.918715  \n",
    "# 6   0.924386  \n",
    "# 7   0.931947  \n",
    "# 8   0.920605  \n",
    "# 9   0.924386  \n",
    "# 10  0.926276  \n",
    "# 11  0.918715  \n",
    "# 12  0.926276  \n",
    "# 13  0.920605  \n",
    "# 14  0.924386  \n",
    "# 15  0.926276  \n",
    "# 16  0.920605  \n",
    "# 17  0.924386  \n",
    "# 18  0.924386  \n",
    "# 19  0.926276  \n",
    "# 20  0.918715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2c03a-5f63-4dbc-8e07-4cf08172abcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sentiment_env)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
