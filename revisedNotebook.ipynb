{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "22d92903-2d3e-4cc7-afb2-75b577deca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rezzahra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/rezzahra/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rezzahra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/rezzahra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from autocorrect import Speller\n",
    "import re\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "pd\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "Spell = Speller()\n",
    "stemmer = PorterStemmer()\n",
    "folder_path = \"./p00_tweets/\"\n",
    "files = ['processedPositive.csv', 'processedNeutral.csv', 'processedNegative.csv']\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "91e0af4e-3aeb-4729-8269-eebdde824421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase, remove numbers, quotes, and strip leading/trailing spaces\n",
    "    text = re.sub(r\"\\d+\", \"\", text.lower()).replace(\"'\", \"\").replace('\"', \"\")\n",
    "    # Add newlines where a comma is not followed by a space\n",
    "    text = re.sub(r\",(?=\\S)\", \"\\n\", text)\n",
    "    # Split by lines and filter out short lines (less than 2 words)\n",
    "    return [line.strip() for line in text.split(\"\\n\") if len(line.split()) >= 2]\n",
    "    \n",
    "def normalize_repeated_letters_and_remove_non_alpha_char(text):\n",
    "    # Replace 3+ repeated letters with a single occurrence\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)  # e.g., \"happyyyy\" -> \"happy\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)   # Remove non-alphabetic characters\n",
    "    return text\n",
    "    \n",
    "negations = {\"not\", \"no\", \"nor\", \"never\"}\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    cleaned_list = [word for word in words if word not in stop_words or word in negations]\n",
    "    return cleaned_list\n",
    "    \n",
    "def count_words(tweet):\n",
    "    return Counter(tweet)\n",
    "\n",
    "def stemmatization(words):\n",
    "    stem_list = [stemmer.stem(word) for word in words]\n",
    "    return stem_list\n",
    "\n",
    "def sentiment_to_numbers(text):\n",
    "    if text == \"positive\":\n",
    "        return 4\n",
    "    elif text == \"negative\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):  # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):  # Verb\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):  # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):  # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to count words for each sentiment category\n",
    "# def get_most_frequent_words(df, sentiment, n=10):\n",
    "#     \"\"\"\n",
    "#     Get the most frequent words in tweets of a specific sentiment.\n",
    "\n",
    "#     Args:\n",
    "#         df (DataFrame): The DataFrame containing 'text' and 'sentiment'.\n",
    "#         sentiment (str): The sentiment category (e.g., 'positive').\n",
    "#         n (int): The number of top words to return.\n",
    "\n",
    "#     Returns:\n",
    "#         List of tuples: Top `n` words and their frequencies.\n",
    "#     \"\"\"\n",
    "#     word_counter = Counter()\n",
    "#     # Filter tweets by sentiment and update word frequencies\n",
    "#     df[df['sentiment'] == sentiment]['lem_word_count'].apply(lambda x: word_counter.update(x))\n",
    "#     return word_counter.most_common(n)\n",
    "\n",
    "# Find the top 10 most frequent words for each sentiment\n",
    "\n",
    "def lemmatize_pos(words):\n",
    "    # words = words.apply(word_tokenize)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    lem_list = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tags]\n",
    "    return lem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "46743a1d-2b1e-41f8-9b36-62e9399912b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = {}\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    # Clean the text and remove duplicates\n",
    "    text = clean_text(raw_text)\n",
    "    processed_text = list(set(text))\n",
    "    # Save processed tweets in the dictionary\n",
    "    key = file.replace(\"processed\", \"\").replace(\".csv\", \"\").lower()\n",
    "    tweets[key] = processed_text\n",
    "\n",
    "    \n",
    "\n",
    "# Output: A dictionary with cleaned and filtered tweets\n",
    "data = []\n",
    "for sentiment, tweet_list in tweets.items():\n",
    "    for tweet in tweet_list:\n",
    "        data.append((tweet, sentiment))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"sentiment\"])\n",
    "\n",
    "# df['lemmatized_text'] = df[\"text\"].apply(lemmatize_pos)\n",
    "\n",
    "# df['lem_word_count'] = df[\"text\"].apply(lemmatize_pos_for_word_count)\n",
    "\n",
    "# positive_words = get_most_frequent_words(df, 'positive', 10)\n",
    "# negative_words = get_most_frequent_words(df, 'negative', 10)\n",
    "# neutral_words = get_most_frequent_words(df, 'neutral', 10)\n",
    "\n",
    "# # Display the results\n",
    "# print(\"Top Positive Words:\", positive_words)\n",
    "# print(\"Top Negative Words:\", negative_words)\n",
    "# print(\"Top Neutral Words:\", neutral_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1215af97-1654-4ebe-9c7e-84f1e8bd142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, technique):\n",
    "    if \"custom preprocessing\" in technique:\n",
    "        text = text.apply(normalize_repeated_letters)\n",
    "    text = text.apply(word_tokenize)\n",
    "    if \"custom preprocessing\" in technique:\n",
    "        text = text.apply(remove_stop_words)\n",
    "    # print(f\"preprocessText: {text}\")\n",
    "    if technique == \"tokenization\":\n",
    "        return text.apply(lambda x: ' '.join(x))\n",
    "    elif technique == \"stemming\":\n",
    "        return text.apply(lambda x: ' '.join(stemmatization(x)))\n",
    "    elif technique == \"lemmatization\":\n",
    "        return text.apply(lambda x: ' '.join(lemmatize_pos(x)))\n",
    "    elif technique == \"stemming + misspellings\" or technique == \"stemming + misspellings + custom preprocessing\":\n",
    "        stemmed = text.apply(lambda x: stemmatization(x))\n",
    "        return text.apply(lambda x: ' '.join([Spell(word) for word in x]))\n",
    "    elif technique == \"lemmatization + misspellings\" or technique == \"lemmatization + misspellings + custom preprocessing\":\n",
    "        lemmatized = text.apply(lambda x: lemmatize_pos(x))\n",
    "        return lemmatized.apply(lambda x: ' '.join([Spell(word) for word in x]))\n",
    "    else:\n",
    "        return text.apply(lambda x: ' '.join(x))\n",
    "\n",
    "def prepare_features(text, method):\n",
    "    if method == \"binary\":\n",
    "        vectorizer = CountVectorizer(binary=True, max_features=5000)\n",
    "    elif method == \"word_counts\":\n",
    "        vectorizer = CountVectorizer(max_features=5000)\n",
    "    elif method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    return vectorizer.fit_transform(text)\n",
    "\n",
    "# def evaluate_model(X, y):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#     model = LogisticRegression(max_iter=5000, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     param_grid = [\n",
    "#     {'penalty':['l1','l2','elasticnet','none'],\n",
    "#     'C' : np.logspace(-4,4,20),\n",
    "#     'solver': ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "#     'max_iter'  : [100,1000,2500,5000]\n",
    "#     }\n",
    "#     ]\n",
    "#     clf = GridSearchCV(model,param_grid = param_grid, cv = 10, verbose=True,n_jobs=-1)\n",
    "#     clf\n",
    "#     best_clf = clf.fit(X,y)\n",
    "#     best_clf.best_estimator_\n",
    "#     print(f'Accuracy - : {best_clf.score(x,y):.3f}')\n",
    "#     y_pred = model.predict(X_test)\n",
    "    \n",
    "#     return accuracy_score(y_test, y_pred)\n",
    "# #     param_grid = {\n",
    "# #     'C': [0.1, 1, 10],\n",
    "# #     'kernel': ['linear', 'rbf'],\n",
    "# #     'gamma': [0.001, 0.01, 0.1]\n",
    "# #     }\n",
    "\n",
    "# # # Initialize GridSearchCV\n",
    "# #     grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # # Train the model\n",
    "# # #     grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # # # Print the best parameters\n",
    "# # #     print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# # # # Evaluate the model with the best parameters\n",
    "# # #     best_model = grid_search.best_estimator_\n",
    "# # #     accuracy = best_model.score(X_test, y_test)\n",
    "# # #     return accuracy    \n",
    "\n",
    "def evaluate_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000,random_state=42)  # saga is better for sparse data\n",
    "    grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "    logreg=LogisticRegression()\n",
    "    logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "    logreg_cv.fit(X_train,y_train)\n",
    "    print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "    print(\"accuracy :\",logreg_cv.best_score_)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "def test_combinations(df):\n",
    "    results = []\n",
    "    techniques = [\"tokenization\", \"stemming\", \"lemmatization\", \"stemming + misspellings\", \"lemmatization + misspellings\",\"lemmatization + misspellings + custom preprocessing\",\"stemming + misspellings + custom preprocessing\"]\n",
    "                  \n",
    "    methods = [\"binary\", \"word_counts\", \"tfidf\"]\n",
    "    \n",
    "    for technique in techniques:\n",
    "        for method in methods:\n",
    "            print(f\"Testing: {technique} + {method}\")\n",
    "            processed_text = preprocess_text(df[\"text\"], technique)\n",
    "            X = prepare_features(processed_text, method)\n",
    "            y = df['sentiment'].apply(sentiment_to_numbers)\n",
    "            accuracy = evaluate_model(X, y)\n",
    "            results.append((technique, method, accuracy))\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "94031188-8461-4d8b-94c3-9ccb5dfd2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: tokenization + binary\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the Test\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_combinations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Display Results\u001b[39;00m\n\u001b[1;32m      6\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing Technique\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Preparation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[107], line 96\u001b[0m, in \u001b[0;36mtest_combinations\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     94\u001b[0m X \u001b[38;5;241m=\u001b[39m prepare_features(processed_text, method)\n\u001b[1;32m     95\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(sentiment_to_numbers)\n\u001b[0;32m---> 96\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m results\u001b[38;5;241m.\u001b[39mappend((technique, method, accuracy))\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[107], line 78\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     76\u001b[0m logreg\u001b[38;5;241m=\u001b[39mLogisticRegression()\n\u001b[1;32m     77\u001b[0m logreg_cv\u001b[38;5;241m=\u001b[39mGridSearchCV(logreg,grid,cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m logreg_cv\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m,y_train)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuned hpyerparameters :(best parameters) \u001b[39m\u001b[38;5;124m\"\u001b[39m,logreg_cv\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy :\u001b[39m\u001b[38;5;124m\"\u001b[39m,logreg_cv\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the Test\n",
    "\n",
    "results = test_combinations(df)\n",
    "\n",
    "# Display Results\n",
    "results_df = pd.DataFrame(results, columns=[\"Preprocessing Technique\", \"Feature Preparation\", \"Accuracy\"])\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
