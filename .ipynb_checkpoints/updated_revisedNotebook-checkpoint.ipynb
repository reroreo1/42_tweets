{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d92903-2d3e-4cc7-afb2-75b577deca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3.tar.gz (23.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1.tar.gz (57.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[45 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[36m\u001b[1m+ meson setup /private/var/folders/b2/_3t8n25n2cn1s3frvmyvvs780000gp/T/pip-install-wp1tk7j4/scipy_c799947323974fbab7fb0309149336dc /private/var/folders/b2/_3t8n25n2cn1s3frvmyvvs780000gp/T/pip-install-wp1tk7j4/scipy_c799947323974fbab7fb0309149336dc/.mesonpy-4l3kizaq -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/private/var/folders/b2/_3t8n25n2cn1s3frvmyvvs780000gp/T/pip-install-wp1tk7j4/scipy_c799947323974fbab7fb0309149336dc/.mesonpy-4l3kizaq/meson-python-native-file.ini\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The Meson build system\n",
      "  \u001b[31m   \u001b[0m Version: 1.7.0\n",
      "  \u001b[31m   \u001b[0m Source dir: /private/var/folders/b2/_3t8n25n2cn1s3frvmyvvs780000gp/T/pip-install-wp1tk7j4/scipy_c799947323974fbab7fb0309149336dc\n",
      "  \u001b[31m   \u001b[0m Build dir: /private/var/folders/b2/_3t8n25n2cn1s3frvmyvvs780000gp/T/pip-install-wp1tk7j4/scipy_c799947323974fbab7fb0309149336dc/.mesonpy-4l3kizaq\n",
      "  \u001b[31m   \u001b[0m Build type: native build\n",
      "  \u001b[31m   \u001b[0m Project name: scipy\n",
      "  \u001b[31m   \u001b[0m Project version: 1.13.1\n",
      "  \u001b[31m   \u001b[0m C compiler for the host machine: cc (clang 16.0.0 \"Apple clang version 16.0.0 (clang-1600.0.26.6)\")\n",
      "  \u001b[31m   \u001b[0m C linker for the host machine: cc ld64 1115.7.3\n",
      "  \u001b[31m   \u001b[0m C++ compiler for the host machine: c++ (clang 16.0.0 \"Apple clang version 16.0.0 (clang-1600.0.26.6)\")\n",
      "  \u001b[31m   \u001b[0m C++ linker for the host machine: c++ ld64 1115.7.3\n",
      "  \u001b[31m   \u001b[0m Cython compiler for the host machine: cython (cython 3.0.12)\n",
      "  \u001b[31m   \u001b[0m Host machine cpu family: aarch64\n",
      "  \u001b[31m   \u001b[0m Host machine cpu: aarch64\n",
      "  \u001b[31m   \u001b[0m Program python found: YES (/Users/reroreo1/Desktop/tweets/.venv/bin/python3.13)\n",
      "  \u001b[31m   \u001b[0m Found pkg-config: YES (/opt/homebrew/bin/pkg-config) 2.3.0\n",
      "  \u001b[31m   \u001b[0m Run-time dependency python found: YES 3.13\n",
      "  \u001b[31m   \u001b[0m Program cython found: YES (/private/var/folders/b2/_3t8n25n2cn1s3frvmyvvs780000gp/T/pip-build-env-j43paxic/overlay/bin/cython)\n",
      "  \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-unused-but-set-variable: YES\n",
      "  \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-unused-function: YES\n",
      "  \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-conversion: YES\n",
      "  \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-misleading-indentation: YES\n",
      "  \u001b[31m   \u001b[0m Library m found: YES\n",
      "  \u001b[31m   \u001b[0m Fortran compiler for the host machine: gfortran (gcc 14.2.0 \"GNU Fortran (Homebrew GCC 14.2.0_1) 14.2.0\")\n",
      "  \u001b[31m   \u001b[0m Fortran linker for the host machine: gfortran ld64 1115.7.3\n",
      "  \u001b[31m   \u001b[0m Compiler for Fortran supports arguments -Wno-conversion: YES\n",
      "  \u001b[31m   \u001b[0m Compiler for C supports link arguments -Wl,-ld_classic: YES\n",
      "  \u001b[31m   \u001b[0m Checking if \"-Wl,--version-script\" links: NO\n",
      "  \u001b[31m   \u001b[0m Program pythran found: YES 0.15.0 0.15.0 (/private/var/folders/b2/_3t8n25n2cn1s3frvmyvvs780000gp/T/pip-build-env-j43paxic/overlay/bin/pythran)\n",
      "  \u001b[31m   \u001b[0m Did not find CMake 'cmake'\n",
      "  \u001b[31m   \u001b[0m Found CMake: NO\n",
      "  \u001b[31m   \u001b[0m Run-time dependency xsimd found: NO (tried pkgconfig, framework and cmake)\n",
      "  \u001b[31m   \u001b[0m Run-time dependency threads found: YES\n",
      "  \u001b[31m   \u001b[0m Library npymath found: YES\n",
      "  \u001b[31m   \u001b[0m Library npyrandom found: YES\n",
      "  \u001b[31m   \u001b[0m pybind11-config found: YES (/private/var/folders/b2/_3t8n25n2cn1s3frvmyvvs780000gp/T/pip-build-env-j43paxic/overlay/bin/pybind11-config) 2.12.1\n",
      "  \u001b[31m   \u001b[0m Run-time dependency pybind11 found: YES 2.12.1\n",
      "  \u001b[31m   \u001b[0m Run-time dependency scipy-openblas found: NO (tried pkgconfig)\n",
      "  \u001b[31m   \u001b[0m Run-time dependency openblas found: NO (tried pkgconfig, framework and cmake)\n",
      "  \u001b[31m   \u001b[0m Run-time dependency openblas found: NO (tried pkgconfig and framework)\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m ../scipy/meson.build:163:9: ERROR: Dependency \"OpenBLAS\" not found, tried pkgconfig and framework\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m A full log can be found at /private/var/folders/b2/_3t8n25n2cn1s3frvmyvvs780000gp/T/pip-install-wp1tk7j4/scipy_c799947323974fbab7fb0309149336dc/.mesonpy-4l3kizaq/meson-logs/meson-log.txt\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install gensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "!pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from autocorrect import Speller\n",
    "import re\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "pd\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "Spell = Speller()\n",
    "stemmer = PorterStemmer()\n",
    "folder_path = \"./p00_tweets/\"\n",
    "files = ['processedPositive.csv', 'processedNeutral.csv', 'processedNegative.csv']\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0af4e-3aeb-4729-8269-eebdde824421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase, remove numbers, quotes, and strip leading/trailing spaces\n",
    "    text = re.sub(r\"\\d+\", \"\", text.lower()).replace(\"'\", \"\").replace('\"', \"\")\n",
    "    # Add newlines where a comma is not followed by a space\n",
    "    text = re.sub(r\",(?=\\S)\", \"\\n\", text)\n",
    "    # Split by lines and filter out short lines (less than 2 words)\n",
    "    return [line.strip() for line in text.split(\"\\n\") if len(line.split()) >= 2]\n",
    "    \n",
    "def normalize_repeated_letters_and_remove_non_alpha_char(text):\n",
    "    # Replace 3+ repeated letters with a single occurrence\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)  # e.g., \"happyyyy\" -> \"happy\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)   # Remove non-alphabetic characters\n",
    "    return text\n",
    "    \n",
    "negations = {\"not\", \"no\", \"nor\", \"never\"}\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    cleaned_list = [word for word in words if word not in stop_words or word in negations]\n",
    "    return cleaned_list\n",
    "    \n",
    "def count_words(tweet):\n",
    "    return Counter(tweet)\n",
    "\n",
    "def stemmatization(words):\n",
    "    stem_list = [stemmer.stem(word) for word in words]\n",
    "    return stem_list\n",
    "\n",
    "def sentiment_to_numbers(text):\n",
    "    if text == \"positive\":\n",
    "        return 4\n",
    "    elif text == \"negative\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):  # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):  # Verb\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):  # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):  # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to count words for each sentiment category\n",
    "# def get_most_frequent_words(df, sentiment, n=10):\n",
    "#     \"\"\"\n",
    "#     Get the most frequent words in tweets of a specific sentiment.\n",
    "\n",
    "#     Args:\n",
    "#         df (DataFrame): The DataFrame containing 'text' and 'sentiment'.\n",
    "#         sentiment (str): The sentiment category (e.g., 'positive').\n",
    "#         n (int): The number of top words to return.\n",
    "\n",
    "#     Returns:\n",
    "#         List of tuples: Top `n` words and their frequencies.\n",
    "#     \"\"\"\n",
    "#     word_counter = Counter()\n",
    "#     # Filter tweets by sentiment and update word frequencies\n",
    "#     df[df['sentiment'] == sentiment]['lem_word_count'].apply(lambda x: word_counter.update(x))\n",
    "#     return word_counter.most_common(n)\n",
    "\n",
    "# Find the top 10 most frequent words for each sentiment\n",
    "\n",
    "def lemmatize_pos(words):\n",
    "    # words = words.apply(word_tokenize)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    lem_list = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tags]\n",
    "    return lem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46743a1d-2b1e-41f8-9b36-62e9399912b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = {}\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    # Clean the text and remove duplicates\n",
    "    text = clean_text(raw_text)\n",
    "    processed_text = list(set(text))\n",
    "    # Save processed tweets in the dictionary\n",
    "    key = file.replace(\"processed\", \"\").replace(\".csv\", \"\").lower()\n",
    "    tweets[key] = processed_text\n",
    "\n",
    "    \n",
    "\n",
    "# Output: A dictionary with cleaned and filtered tweets\n",
    "data = []\n",
    "for sentiment, tweet_list in tweets.items():\n",
    "    for tweet in tweet_list:\n",
    "        data.append((tweet, sentiment))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"sentiment\"])\n",
    "\n",
    "# df['lemmatized_text'] = df[\"text\"].apply(lemmatize_pos)\n",
    "\n",
    "# df['lem_word_count'] = df[\"text\"].apply(lemmatize_pos_for_word_count)\n",
    "\n",
    "# positive_words = get_most_frequent_words(df, 'positive', 10)\n",
    "# negative_words = get_most_frequent_words(df, 'negative', 10)\n",
    "# neutral_words = get_most_frequent_words(df, 'neutral', 10)\n",
    "\n",
    "# # Display the results\n",
    "# print(\"Top Positive Words:\", positive_words)\n",
    "# print(\"Top Negative Words:\", negative_words)\n",
    "# print(\"Top Neutral Words:\", neutral_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215af97-1654-4ebe-9c7e-84f1e8bd142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, technique):\n",
    "    if \"custom preprocessing\" in technique:\n",
    "        text = text.apply(normalize_repeated_letters_and_remove_non_alpha_char)\n",
    "    text = text.apply(word_tokenize)\n",
    "    if \"custom preprocessing\" in technique:\n",
    "        text = text.apply(remove_stop_words)\n",
    "    # print(f\"preprocessText: {text}\")\n",
    "    if technique == \"tokenization\":\n",
    "        return text.apply(lambda x: ' '.join(x))\n",
    "    elif technique == \"stemming\":\n",
    "        return text.apply(lambda x: ' '.join(stemmatization(x)))\n",
    "    elif technique == \"lemmatization\":\n",
    "        return text.apply(lambda x: ' '.join(lemmatize_pos(x)))\n",
    "    elif technique == \"stemming + misspellings\" or technique == \"stemming + misspellings + custom preprocessing\":\n",
    "        stemmed = text.apply(lambda x: stemmatization(x))\n",
    "        return text.apply(lambda x: ' '.join([Spell(word) for word in x]))\n",
    "    elif technique == \"lemmatization + misspellings\" or technique == \"lemmatization + misspellings + custom preprocessing\":\n",
    "        lemmatized = text.apply(lambda x: lemmatize_pos(x))\n",
    "        return lemmatized.apply(lambda x: ' '.join([Spell(word) for word in x]))\n",
    "    else:\n",
    "        return text.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# def prepare_features(text, method):\n",
    "#     if method == \"binary\":\n",
    "#         vectorizer = CountVectorizer(binary=True, max_features=5000)\n",
    "#     elif method == \"word_counts\":\n",
    "#         vectorizer = CountVectorizer(max_features=5000)\n",
    "#     elif method == \"tfidf\":\n",
    "#         vectorizer = TfidfVectorizer(max_features=5000)\n",
    "#     return vectorizer.fit_transform(text)\n",
    "\n",
    "   \n",
    "# def evaluate_model(X, y):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     logreg = LogisticRegression(max_iter=5000,random_state=42)  # saga is better for sparse data\n",
    "#     param_grid = [{'C': 10**np.linspace(-3, 3, 20), 'solver': ['lbfgs', 'saga', 'liblinear']}]\n",
    "#     logreg_cv=GridSearchCV(logreg,param_grid, cv=10, scoring='accuracy', \n",
    "#                              refit=True)\n",
    "#     logreg_cv.fit(X_train,y_train)\n",
    "#     print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "#     y_pred = logreg_cv.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     print(f\"accuracy_score function - Accuracy: {accuracy}\")\n",
    "#     return accuracy\n",
    "\n",
    "    \n",
    "# def test_combinations(df):\n",
    "#     results = []\n",
    "#     techniques = [\"tokenization\", \"stemming\", \"lemmatization\", \"stemming + misspellings\", \"lemmatization + misspellings\",\"lemmatization + misspellings + custom preprocessing\",\"stemming + misspellings + custom preprocessing\"]\n",
    "                  \n",
    "#     methods = [\"binary\", \"word_counts\", \"tfidf\"]\n",
    "    \n",
    "#     for technique in techniques:\n",
    "#         for method in methods:\n",
    "#             print(f\"Testing: {technique} + {method}\")\n",
    "#             processed_text = preprocess_text(df[\"text\"], technique)\n",
    "#             X = prepare_features(processed_text, method)\n",
    "#             y = df['sentiment'].apply(sentiment_to_numbers)\n",
    "#             accuracy = evaluate_model(X, y)\n",
    "#             results.append((technique, method, accuracy))\n",
    "#             print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "#     return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94031188-8461-4d8b-94c3-9ccb5dfd2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Test\n",
    "\n",
    "# results = test_combinations(df)\n",
    "\n",
    "# # Display Results\n",
    "# results_df = pd.DataFrame(results, columns=[\"Preprocessing Technique\", \"Feature Preparation\", \"Accuracy\"])\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f036c2-6f69-4c93-8c84-dfb5e0175a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: tokenization + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(0.6951927961775606)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: tokenization + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: tokenization + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(6.158482110660261)}\n",
    "# accuracy_score function - Accuracy: 0.9168241965973535\n",
    "# Accuracy: 0.9168\n",
    "# Testing: stemming + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(0.6951927961775606)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: stemming + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9187145557655955\n",
    "# Accuracy: 0.9187\n",
    "# Testing: stemming + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(12.742749857031322)}\n",
    "# accuracy_score function - Accuracy: 0.9187145557655955\n",
    "# Accuracy: 0.9187\n",
    "# Testing: lemmatization + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: lemmatization + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(0.6951927961775606)}\n",
    "# accuracy_score function - Accuracy: 0.9319470699432892\n",
    "# Accuracy: 0.9319\n",
    "# Testing: lemmatization + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(233.57214690901213)}\n",
    "# accuracy_score function - Accuracy: 0.9206049149338374\n",
    "# Accuracy: 0.9206\n",
    "# Testing: stemming + misspellings + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: stemming + misspellings + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: stemming + misspellings + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9187145557655955\n",
    "# Accuracy: 0.9187\n",
    "# Testing: lemmatization + misspellings + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: lemmatization + misspellings + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(6.158482110660261)}\n",
    "# accuracy_score function - Accuracy: 0.9206049149338374\n",
    "# Accuracy: 0.9206\n",
    "# Testing: lemmatization + misspellings + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1000.0)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: lemmatization + misspellings + custom preprocessing + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: lemmatization + misspellings + custom preprocessing + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(6.158482110660261)}\n",
    "# accuracy_score function - Accuracy: 0.9206049149338374\n",
    "# Accuracy: 0.9206\n",
    "# Testing: lemmatization + misspellings + custom preprocessing + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1000.0)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: stemming + misspellings + custom preprocessing + binary\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9243856332703214\n",
    "# Accuracy: 0.9244\n",
    "# Testing: stemming + misspellings + custom preprocessing + word_counts\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(1.438449888287663)}\n",
    "# accuracy_score function - Accuracy: 0.9262759924385633\n",
    "# Accuracy: 0.9263\n",
    "# Testing: stemming + misspellings + custom preprocessing + tfidf\n",
    "# tuned hpyerparameters :(best parameters)  {'C': np.float64(2.976351441631316)}\n",
    "# accuracy_score function - Accuracy: 0.9187145557655955\n",
    "# Accuracy: 0.9187\n",
    "#                               Preprocessing Technique Feature Preparation  \\\n",
    "# 0                                        tokenization              binary   \n",
    "# 1                                        tokenization         word_counts   \n",
    "# 2                                        tokenization               tfidf   \n",
    "# 3                                            stemming              binary   \n",
    "# 4                                            stemming         word_counts   \n",
    "# 5                                            stemming               tfidf   \n",
    "# 6                                       lemmatization              binary   \n",
    "# 7                                       lemmatization         word_counts   \n",
    "# 8                                       lemmatization               tfidf   \n",
    "# 9                             stemming + misspellings              binary   \n",
    "# 10                            stemming + misspellings         word_counts   \n",
    "# 11                            stemming + misspellings               tfidf   \n",
    "# 12                       lemmatization + misspellings              binary   \n",
    "# 13                       lemmatization + misspellings         word_counts   \n",
    "# 14                       lemmatization + misspellings               tfidf   \n",
    "# 15  lemmatization + misspellings + custom preproce...              binary   \n",
    "# 16  lemmatization + misspellings + custom preproce...         word_counts   \n",
    "# 17  lemmatization + misspellings + custom preproce...               tfidf   \n",
    "# 18     stemming + misspellings + custom preprocessing              binary   \n",
    "# 19     stemming + misspellings + custom preprocessing         word_counts   \n",
    "# 20     stemming + misspellings + custom preprocessing               tfidf   \n",
    "\n",
    "#     Accuracy  \n",
    "# 0   0.926276  \n",
    "# 1   0.924386  \n",
    "# 2   0.916824  \n",
    "# 3   0.926276  \n",
    "# 4   0.918715  \n",
    "# 5   0.918715  \n",
    "# 6   0.924386  \n",
    "# 7   0.931947  \n",
    "# 8   0.920605  \n",
    "# 9   0.924386  \n",
    "# 10  0.926276  \n",
    "# 11  0.918715  \n",
    "# 12  0.926276  \n",
    "# 13  0.920605  \n",
    "# 14  0.924386  \n",
    "# 15  0.926276  \n",
    "# 16  0.920605  \n",
    "# 17  0.924386  \n",
    "# 18  0.924386  \n",
    "# 19  0.926276  \n",
    "# 20  0.918715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2c03a-5f63-4dbc-8e07-4cf08172abcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec(sentences, vector_size=100, window=5, min_count=2):\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4)\n",
    "    return model\n",
    "\n",
    "def get_average_word2vec(sentences, model, vector_size):\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "        if word_vectors:\n",
    "            vectors.append(sum(word_vectors) / len(word_vectors))\n",
    "        else:\n",
    "            vectors.append([0] * vector_size)\n",
    "    return np.array(vectors)\n",
    "\n",
    "def prepare_features(text, method):\n",
    "    if method == \"binary\":\n",
    "        vectorizer = CountVectorizer(binary=True, max_features=5000)\n",
    "        return vectorizer.fit_transform(text)\n",
    "    elif method == \"word_counts\":\n",
    "        vectorizer = CountVectorizer(max_features=5000)\n",
    "        return vectorizer.fit_transform(text)\n",
    "    elif method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        return vectorizer.fit_transform(text)\n",
    "    elif method == \"word2vec\":\n",
    "        tokenized_text = [sentence.split() for sentence in text]\n",
    "        w2v_model = train_word2vec(tokenized_text)\n",
    "        return get_average_word2vec(text, w2v_model, vector_size=100)\n",
    "\n",
    "def test_combinations(df):\n",
    "    results = []\n",
    "    techniques = [\"tokenization\", \"stemming\", \"lemmatization\", \"stemming + misspellings\", \"lemmatization + misspellings\",\"lemmatization + misspellings + custom preprocessing\",\"stemming + misspellings + custom preprocessing\"]\n",
    "    methods = [\"binary\", \"word_counts\", \"tfidf\", \"word2vec\"]  # Added Word2Vec\n",
    "\n",
    "    for technique in techniques:\n",
    "        for method in methods:\n",
    "            print(f\"Testing: {technique} + {method}\")\n",
    "            processed_text = preprocess_text(df[\"text\"], technique)\n",
    "            X = prepare_features(processed_text, method)\n",
    "            y = df['sentiment'].apply(sentiment_to_numbers)\n",
    "            accuracy = evaluate_model(X, y)\n",
    "            results.append((technique, method, accuracy))\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sentiment_env)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
